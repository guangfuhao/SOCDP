{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d41a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Set the current GPU device to only device number 0, with device name '/gpu: 0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Set the log output information, which is the information printed by the system when the program is running. Display only warning and error\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available!')\n",
    "else:\n",
    "    print('CUDA is not available!')\n",
    "\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from models import *\n",
    "from dataset import *\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09ae9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def generate_file_path(out_dir, middle_size, class_num, model_name, train_idx):\n",
    "    path = get_path_metrics(out_dir+\"loss_acc\", middle_size, class_num, model_name, EPOCH, train_idx, extension=f\".pickle\")\n",
    "    return path\n",
    "\n",
    "def load_data_from_file(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def compute_stats(data):\n",
    "    data = np.array(data)\n",
    "    avg = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    var_up = avg - std\n",
    "    var_lower = avg + std\n",
    "    return avg, var_up, var_lower\n",
    "\n",
    "def evaluate_history_contrast_loss_train_test(out_dir, middle_size, class_num, num_times=100):\n",
    "    loss_train_test_all1, loss_train_test_all2, loss_train_test_all3, loss_train_test_all4, loss_train_test_all5 = [], [], [], [], []\n",
    "    for i in range(1, num_times + 1):\n",
    "        path1 = generate_file_path(out_dir, middle_size, class_num, MODEL_NAME[0], i)\n",
    "        path2 = generate_file_path(out_dir, middle_size, class_num, MODEL_NAME[1], i)\n",
    "        path3 = generate_file_path(out_dir, middle_size, class_num, MODEL_NAME[2], i)\n",
    "        path4 = generate_file_path(out_dir, middle_size, class_num, MODEL_NAME[3], i)\n",
    "        path5 = generate_file_path(out_dir, middle_size, class_num, MODEL_NAME[4], i)\n",
    "\n",
    "        loss_train_test_all1.append(load_data_from_file(path1))\n",
    "        loss_train_test_all2.append(load_data_from_file(path2))\n",
    "        loss_train_test_all3.append(load_data_from_file(path3))\n",
    "        loss_train_test_all4.append(load_data_from_file(path4))\n",
    "        loss_train_test_all5.append(load_data_from_file(path5))\n",
    "\n",
    "    avg1, var_up_1, var_lower_1 = compute_stats(loss_train_test_all1)\n",
    "    avg2, var_up_2, var_lower_2 = compute_stats(loss_train_test_all2)\n",
    "    avg3, var_up_3, var_lower_3 = compute_stats(loss_train_test_all3)\n",
    "    avg4, var_up_4, var_lower_4 = compute_stats(loss_train_test_all4)\n",
    "    avg5, var_up_5, var_lower_5 = compute_stats(loss_train_test_all5)\n",
    "\n",
    "    data_to_save = {\n",
    "        MODEL_NAME[0]: [loss_train_test_all1, avg1, var_up_1, var_lower_1],\n",
    "        MODEL_NAME[1]: [loss_train_test_all2, avg2, var_up_2, var_lower_2],\n",
    "        MODEL_NAME[2]: [loss_train_test_all3, avg3, var_up_3, var_lower_3],\n",
    "        MODEL_NAME[3]: [loss_train_test_all4, avg4, var_up_4, var_lower_4],\n",
    "        MODEL_NAME[4]: [loss_train_test_all5, avg5, var_up_5, var_lower_5],\n",
    "    }\n",
    "    # Format filename with parameters\n",
    "    file_name = f\"./result/loss_accuracy/all_data_middle_size_{middle_size}_class_num_{class_num}_times{num_times}.pkl\"\n",
    "    ensure_dir_exists(\"./result/loss_accuracy/\")\n",
    "\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(data_to_save, file)\n",
    "\n",
    "    return data_to_save\n",
    "\n",
    "from textwrap import wrap\n",
    "\n",
    "def plot_variance_comparison_legend_center_right(data, metric_index, metric_name, ax):\n",
    "    epochs = np.arange(1, 11)\n",
    "    colors = ['skyblue', 'orange', 'green', 'red', 'purple']  # Distinct colors for each CNN type\n",
    "    for idx, (cnn_type, color) in enumerate(zip(data.keys(), colors)):\n",
    "        lower_bounds = data[cnn_type][2][metric_index]\n",
    "        upper_bounds = data[cnn_type][3][metric_index]\n",
    "        ax.fill_between(epochs, lower_bounds, upper_bounds, color=color, alpha=0.3, label=f'{cnn_type} Bounds')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(f'{metric_name} Bounds')\n",
    "    title = f'Comparison of {metric_name} Variance Bounds Across CNN Types'\n",
    "    ax.set_title(\"\\n\".join(wrap(title, 40)))  # Wrap title based on the plot width\n",
    "    ax.legend(loc='center right')  # Move legend to the center right\n",
    "\n",
    "def plot_fig0_bounds_comparison_loss_train_test(out_dir, middle_size, class_num, num_times=100):\n",
    "    # Create a figure with subplots for each metric's variance comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    # Iterate over each metric to create the plots with error bars\n",
    "    metrics = ['Loss', 'Training Accuracy', 'Validation Accuracy']\n",
    "    data_instance = evaluate_history_contrast_loss_train_test(out_dir=out_dir, middle_size=middle_size, class_num=class_num, num_times=num_times)\n",
    "    for i, metric_name in enumerate(metrics):\n",
    "        plot_variance_comparison_legend_center_right(data_instance, i, metric_name, axes[i])\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_variance_comparison_error_bars(data, metric_index, metric_name, ax):\n",
    "    epochs = np.arange(1, EPOCH+1)\n",
    "    colors = ['skyblue', 'orange', 'green', 'red', 'purple']  # Distinct colors for each CNN type\n",
    "    for idx, (cnn_type, color) in enumerate(zip(data.keys(), colors)):\n",
    "        means = data[cnn_type][1][metric_index]\n",
    "        lower_bounds = data[cnn_type][2][metric_index]\n",
    "        upper_bounds = data[cnn_type][3][metric_index]\n",
    "        error = [means - lower_bounds, upper_bounds - means]  # Asymmetric error bars\n",
    "        ax.errorbar(epochs, means, yerr=error, fmt='-o', color=color, ecolor=color, elinewidth=2, capsize=4, label=f'{cnn_type}')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(f'{metric_name} with Variance')\n",
    "    title = f'Comparison of {metric_name} with Variance Across CNN Types'\n",
    "    ax.set_title(\"\\n\".join(wrap(title, 40)))  # Wrap title based on the plot width\n",
    "    ax.legend(loc='center right')  # Move legend to the center right\n",
    "    ax.grid(True)  # Enable grid lines\n",
    "\n",
    "\n",
    "def plot_fig1_loss_train_test(out_dir, middle_size, class_num, num_times=100):\n",
    "    # Create a figure with subplots for each metric's variance comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    # Iterate over each metric to create the plots with error bars\n",
    "    metrics = ['Loss', 'Training Accuracy', 'Validation Accuracy']\n",
    "    data_instance = evaluate_history_contrast_loss_train_test(out_dir=out_dir, middle_size=middle_size, class_num=class_num, num_times=num_times)\n",
    "    for i, metric_name in enumerate(metrics):\n",
    "        plot_variance_comparison_error_bars(data_instance, i, metric_name, axes[i])\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_final_epoch_comparison_adjusted(data, metric_index, metric_name, ax, y_limit=None):\n",
    "    final_epoch_index = EPOCH-1  # Zero-based indexing for the 10th epoch\n",
    "    means = [data[cnn_type][1][metric_index, final_epoch_index] for cnn_type in data.keys()]\n",
    "    cnn_types = list(data.keys())\n",
    "    colors = ['skyblue', 'orange', 'green', 'red', 'purple']  # Distinct colors for each CNN type\n",
    "    alpha=0.5\n",
    "    \n",
    "    # ax.bar(cnn_types, means, color='skyblue')\n",
    "    ax.bar(cnn_types, means, color=colors, alpha=alpha)\n",
    "    ax.set_xlabel('CNN Types')\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.set_title(f'Comparison of {metric_name} at Final Epoch')\n",
    "    ax.set_xticklabels(cnn_types, rotation=45, ha='right')\n",
    "    if y_limit:\n",
    "        ax.set_ylim(y_limit)\n",
    "\n",
    "\n",
    "def plot_fig2_bar_plots_final_epoch(out_dir, middle_size, class_num, num_times=100):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    metrics = ['Loss', 'Training Accuracy', 'Validation Accuracy']\n",
    "    data_instance = evaluate_history_contrast_loss_train_test(out_dir=out_dir, middle_size=middle_size, class_num=class_num, num_times=num_times)\n",
    "    for i, metric_name in enumerate(metrics):\n",
    "        y_limit = (0.9, 1.0) if metric_name != 'Loss' else None\n",
    "        plot_final_epoch_comparison_adjusted(data_instance, i, metric_name, axes[i], y_limit=y_limit)\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_metrics_autowrap(data, cnn_type, metric_index, metric_name, sub_plot_index, ax, include_bounds=False):\n",
    "    colors = ['skyblue', 'orange', 'green', 'red', 'purple']  # Distinct colors for each CNN type\n",
    "    observations = np.array(data[cnn_type][0])[:, metric_index, :]\n",
    "    means = data[cnn_type][1][metric_index]\n",
    "    lower_bounds = data[cnn_type][2][metric_index]\n",
    "    upper_bounds = data[cnn_type][3][metric_index]\n",
    "    epochs = np.arange(1, 11)\n",
    "\n",
    "    ax.plot(epochs, means, label=f'{cnn_type} Mean {metric_name}', marker='o', color=colors[sub_plot_index])\n",
    "\n",
    "    if not include_bounds:\n",
    "        for obs in observations:\n",
    "            ax.plot(epochs, obs, linestyle='--', alpha=0.3, color=colors[sub_plot_index])\n",
    "\n",
    "    if include_bounds:\n",
    "        ax.fill_between(epochs, lower_bounds, upper_bounds, alpha=0.2, label=f'{cnn_type} {metric_name} Bounds', color=colors[sub_plot_index])\n",
    "\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(metric_name)\n",
    "    title = f'{cnn_type} {metric_name} Across Epochs'\n",
    "    ax.set_title(\"\\n\".join(wrap(title, 40)))  # Wrap title based on the plot width\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def plot_fig3_plots_the_metric_across_epochs(out_dir, middle_size, class_num, num_times=100):\n",
    "    fig, axes = plt.subplots(5, 3, figsize=(15, 25), sharex=True, sharey='row')\n",
    "    metrics = ['Loss', 'Training Accuracy', 'Validation Accuracy']\n",
    "    data_instance = evaluate_history_contrast_loss_train_test(out_dir=out_dir, middle_size=middle_size, class_num=class_num, num_times=num_times)\n",
    "    cnn_types = list(data_instance.keys())\n",
    "    for i, cnn_type in enumerate(cnn_types):\n",
    "        for j, metric_name in enumerate(metrics):\n",
    "            plot_metrics_autowrap(data_instance, cnn_type, j, metric_name, i, axes[i, j], include_bounds=(j == 0))\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_metric_distribution_autowrap(data, cnn_type, metric_index, metric_name, sub_plot_index, ax):\n",
    "    colors = ['skyblue', 'orange', 'green', 'red', 'purple']  # Distinct colors for each CNN type\n",
    "    observations = np.array(data[cnn_type][0])[:, metric_index, :]\n",
    "    box = ax.boxplot(observations, labels=[f'Epoch {i+1}' for i in range(observations.shape[1])])\n",
    "    for boxs in box['boxes']:\n",
    "        # Set edge color\n",
    "        boxs.set(color=colors[sub_plot_index])\n",
    "    for median in box['medians']:\n",
    "        median.set_color(colors[sub_plot_index])\n",
    "\n",
    "\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(metric_name)\n",
    "    title = f'Distribution of {metric_name} Across Epochs for {cnn_type}'\n",
    "    ax.set_title(\"\\n\".join(wrap(title, 40)))  # Wrap title based on the plot width\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "\n",
    "def plot_fig4_box_plots_the_metric_across_epochs(out_dir, middle_size, class_num, num_times=100):\n",
    "    fig, axes = plt.subplots(5, 3, figsize=(15, 25), sharex=True, sharey='row')\n",
    "    metrics = ['Loss', 'Training Accuracy', 'Validation Accuracy']\n",
    "    data_instance = evaluate_history_contrast_loss_train_test(out_dir=out_dir, middle_size=middle_size, class_num=class_num, num_times=num_times)\n",
    "    cnn_types = list(data_instance.keys())\n",
    "    for i, cnn_type in enumerate(cnn_types):\n",
    "        for j, metric_name in enumerate(metrics):\n",
    "            plot_metric_distribution_autowrap(data_instance, cnn_type, j, metric_name, i, axes[i, j])\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_without_number_display_images(disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/data0/user/gfhao/code_cpm/ServerCode/output/\"\n",
    "middle_size = 16\n",
    "class_num = 20\n",
    "num_times = 100\n",
    "fig1 = plot_fig1_loss_train_test(out_dir, middle_size, class_num, num_times)\n",
    "fig2 = plot_fig2_bar_plots_final_epoch(out_dir, middle_size, class_num, num_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "def generate_file_path_model(out_dir, middle_size, class_num, model_name, train_idx, epoch):\n",
    "    filename = f\"model_{train_idx}_epoch_{epoch}.pt\"\n",
    "    path = get_path(out_dir+\"model_weights\", middle_size, class_num, model_name, filename)\n",
    "    return path\n",
    "\n",
    "\n",
    "def generate_fid_path(middle_size, class_num, num_times):\n",
    "    path = f\"/data0/user/gfhao/code_cpm/ServerCode/result/fid/fid_data_middle_size_{middle_size}_class_num_{class_num}_times{num_times}.pkl\"\n",
    "    return path\n",
    "\n",
    "\n",
    "def load_data_from_file(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def get_history_fid(out_dir, middle_size, class_num, num_times=100):\n",
    "    model_config = [8, 16, middle_size, middle_size * middle_size // 4]\n",
    "    # print(\"middle size:\", middle_size)\n",
    "    fid1, fid2, fid3, fid4, fid5 = [], [], [], [], []\n",
    "    epoch = EPOCH-1\n",
    "    train_data, test_data = load_processed_dataset(data_dir=\"/data0/user/gfhao/datasets/mnist\", batch_size=64, display=False, process=True, class_num=class_num)\n",
    "    # print(\"datasets loaded!\")\n",
    "    for i in tqdm(range(1, num_times + 1), desc=\"Progress\"):\n",
    "        fid_all1, fid_matrix_all1 = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[0], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "        fid_all2, fid_matrix_all2 = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[1], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "        fid_all3, fid_matrix_all3 = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[2], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "        fid_all4, fid_matrix_all4 = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[3], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "        fid_all5, fid_matrix_all5 = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[4], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "\n",
    "        fid1.append([fid_all1, fid_matrix_all1])\n",
    "        fid2.append([fid_all2, fid_matrix_all2])\n",
    "        fid3.append([fid_all3, fid_matrix_all3])\n",
    "        fid4.append([fid_all4, fid_matrix_all4])\n",
    "        fid5.append([fid_all5, fid_matrix_all5])\n",
    "\n",
    "    data_to_save = {\n",
    "        MODEL_NAME[0]: fid1,\n",
    "        MODEL_NAME[1]: fid2,\n",
    "        MODEL_NAME[2]: fid3,\n",
    "        MODEL_NAME[3]: fid4,\n",
    "        MODEL_NAME[4]: fid5,\n",
    "    }\n",
    "    # Format filename with parameters\n",
    "    file_name = f\"./result/fid/fid_data_middle_size_{middle_size}_class_num_{class_num}_times{num_times}.pkl\"\n",
    "    ensure_dir_exists(\"./result/fid/\")\n",
    "\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(data_to_save, file)\n",
    "    \n",
    "    return data_to_save\n",
    "\n",
    "\n",
    "def analyze_fid(middle_size, class_num, num_times):\n",
    "    opposite_sign_counts = {\"middle_size\":round(middle_size, 0), \"class_num\":round(class_num, 0)}\n",
    "    fid_path = generate_fid_path(middle_size, class_num, num_times)\n",
    "    fid_data = load_data_from_file(fid_path)\n",
    "    cnn_types = list(fid_data.keys())\n",
    "    for i, cnn_type in enumerate(cnn_types):\n",
    "        for j in range(0, num_times):\n",
    "            fid_all = fid_data[cnn_type][j][0]\n",
    "            # 计算两个差值\n",
    "            diff1 = round(fid_all[0] - fid_all[1], 2)\n",
    "            diff2 = round(fid_all[2] - fid_all[3], 2)\n",
    "            if (diff1 * diff2) < 0:\n",
    "                if cnn_type in opposite_sign_counts:\n",
    "                    opposite_sign_counts[cnn_type] += 1\n",
    "                else:\n",
    "                    opposite_sign_counts[cnn_type] = 1\n",
    "            else:\n",
    "                if cnn_type not in opposite_sign_counts:\n",
    "                    opposite_sign_counts[cnn_type] = 0\n",
    "    # percentages = {k: (v / num_times) * 100 for k, v in opposite_sign_counts.items()}\n",
    "    percentages = {k: v for k, v in opposite_sign_counts.items()}\n",
    "    return percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/data0/user/gfhao/code_cpm/ServerCode/output/\"\n",
    "num_times = 100\n",
    "\n",
    "class_num_runs = [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50]\n",
    "class_num_runs = [x * 10 for x in class_num_runs]\n",
    "for middle_size in [16,32,64,128,256]:\n",
    "    for class_num_run in class_num_runs:\n",
    "        print(\"class num: \", class_num_run)\n",
    "        get_history_fid(out_dir, middle_size, class_num_run, num_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_fid_percentages(middle_size, num_times, class_num_runs):\n",
    "    percentages_per_middle_size = []\n",
    "    for class_num_run in class_num_runs:\n",
    "        percentages = analyze_fid(middle_size, class_num_run, num_times)\n",
    "        percentages_per_middle_size.append(percentages)\n",
    "    # print(percentages_per_middle_size)\n",
    "    import matplotlib.pyplot as plt\n",
    "    model_data = {\n",
    "        'SimpleCNN': [],\n",
    "        'DotProductCNN': [],\n",
    "        'CrossProductCNN': [],\n",
    "        'DotProductSparseCNN': [],\n",
    "        'CrossProductSparseCNN': []\n",
    "    }\n",
    "    # colors = ['skyblue', 'orange', 'green', 'red', 'purple']  # Distinct \n",
    "    class_nums = class_num_runs\n",
    "    for data in percentages_per_middle_size:\n",
    "        for model in model_data.keys():\n",
    "            model_data[model].append(data[model])\n",
    "    fid_of_middle_size_data = {'middle_size_'+str(middle_size):model_data}\n",
    "    return fid_of_middle_size_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "num_times = 100\n",
    "class_num_runs = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50]\n",
    "class_num_runs = [x * 10 for x in class_num_runs]\n",
    "middle_sizes = [16, 32, 64, 128, 256]\n",
    "fids_all = []\n",
    "file_name = f\"./result/fid_all/all_fid_data.pkl\"\n",
    "ensure_dir_exists(\"./result/fid_all/\")\n",
    "for middle_size in middle_sizes:\n",
    "    fids_all.append(cal_fid_percentages(middle_size, num_times, class_num_runs))\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(fids_all, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def load_data_from_file(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def plot_fid_percentages_in_one_fig():\n",
    "    file_name = f\"./result/fid_all/all_fid_data.pkl\"\n",
    "    fid_data = load_data_from_file(file_name)\n",
    "    class_num_runs = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50]\n",
    "    class_num_runs = [x * 10 for x in class_num_runs]\n",
    "    class_nums = class_num_runs\n",
    "    break_point = 200\n",
    "    adjusted_class_nums = []\n",
    "    for num in class_nums:\n",
    "        if num < break_point:\n",
    "            adjusted_class_nums.append(num)\n",
    "        else:\n",
    "            adjusted_class_nums.append(int(break_point + (num - break_point) / 2))\n",
    "    df_data = []\n",
    "    for element in fid_data:\n",
    "        for middle_size, models in element.items():\n",
    "            for model, percentages in models.items():\n",
    "                for class_num_index, percentage in enumerate(percentages):\n",
    "                    df_data.append({\n",
    "                        'Middle Size': middle_size,\n",
    "                        'Model': model,\n",
    "                        'Class Number': (class_num_index + 2)*10,  # Starting from class_num 2\n",
    "                        'Percentage': percentage\n",
    "                    })\n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame(df_data)\n",
    "    # Setting up the figure for multiple subplots - one for each middle size\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(20, 5 * 5), sharey=True)\n",
    "    # fig, axes = plt.subplots(5, 1, sharey=True)\n",
    "    # New colors for different models\n",
    "    model_colors = ['skyblue', 'orange', 'green', 'red', 'purple']\n",
    "    model_color_map = {model: color for model, color in zip(df['Model'].unique(), model_colors)}\n",
    "    # Creating line charts for each middle size\n",
    "    for i, middle_size in enumerate(df['Middle Size'].unique()):\n",
    "        ax = axes[i]\n",
    "        middle_size_data = df[df['Middle Size'] == middle_size]\n",
    "        for model in df['Model'].unique():\n",
    "            model_data = middle_size_data[middle_size_data['Model'] == model]\n",
    "            ax.plot(adjusted_class_nums, model_data['Percentage'], label=model, color=model_color_map[model], marker='o')\n",
    "\n",
    "        # Setting the title for each subplot\n",
    "        ax.set_title(f'Middle Size: {middle_size}')\n",
    "\n",
    "        # Adding x-axis labels (class numbers) for the last subplot\n",
    "        ax.set_xticks(adjusted_class_nums)\n",
    "        ax.set_xticklabels(adjusted_class_nums, rotation=45)\n",
    "        if i == 0:\n",
    "            ax.legend()\n",
    "\n",
    "    # General titles and labels\n",
    "    plt.suptitle('Line Charts of Percentage Values by Class Number (Scaled), Model, and Middle Size')\n",
    "    fig.text(0.5, 0.04, 'Class Number', ha='center', va='center')\n",
    "    fig.text(0.06, 0.5, 'Percentage', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "    # Adjusting layout\n",
    "    plt.tight_layout(rect=[0.03, 0.05, 1, 0.97])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_all_fig = plot_fid_percentages_in_one_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data_from_file(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def plot_fid_percentages_in_one_fig_all():\n",
    "    file_name = f\"./result/fid_all/all_fid_data.pkl\"\n",
    "    fid_data = load_data_from_file(file_name)\n",
    "    class_num_runs = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50]\n",
    "    class_num_runs = [x * 10 for x in class_num_runs]\n",
    "    class_nums = class_num_runs\n",
    "    break_point = 200\n",
    "    adjusted_class_nums = [num if num < break_point else int(break_point + (num - break_point) / 2) for num in class_num_runs]\n",
    "    df_data = []\n",
    "    for element in fid_data:\n",
    "        for middle_size, models in element.items():\n",
    "            for model, percentages in models.items():\n",
    "                if model in ['SimpleCNN', 'DotProductCNN', 'CrossProductCNN']:\n",
    "                    for class_num_index, percentage in enumerate(percentages):\n",
    "                        df_data.append({\n",
    "                            'Middle Size': middle_size,\n",
    "                            'Model': model,\n",
    "                            'Class Number': (class_num_index + 2)*10,  # Starting from class_num 2\n",
    "                            'Percentage': percentage\n",
    "                        })\n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame(df_data)\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 12), sharex=True)\n",
    "\n",
    "    # Models to plot\n",
    "    models = ['DotProductCNN', 'CrossProductCNN']\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        ax = axes[i]\n",
    "        model_data = df[df['Model'] == model]\n",
    "\n",
    "        # Use a colormap for different middle sizes\n",
    "        cmap = plt.cm.get_cmap('Accent', len(model_data['Middle Size'].unique()))\n",
    "\n",
    "        for j, middle_size in enumerate(sorted(model_data['Middle Size'].unique())):\n",
    "            middle_size_data = model_data[model_data['Middle Size'] == middle_size]\n",
    "            ax.plot(adjusted_class_nums, middle_size_data['Percentage'], label=middle_size, color=cmap(j), marker='o')\n",
    "\n",
    "        ax.set_title(f'FID Percentages for {model}')\n",
    "        ax.set_xlabel('Class Number')\n",
    "        ax.set_ylabel('Percentage')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Adding overall titles and labels\n",
    "    plt.suptitle('FID Percentages by Class Number and Middle Size', fontsize=16)\n",
    "    plt.xticks(adjusted_class_nums, adjusted_class_nums, rotation=45)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_fid_percentages_in_one_fig_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/data0/user/gfhao/code_cpm/ServerCode/output/\"\n",
    "\n",
    "middle_size = 16\n",
    "model_config = [8, 16, middle_size, middle_size * middle_size // 4]\n",
    "print(\"middle size:\", middle_size)\n",
    "# class_num_runs = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50]\n",
    "class_num_runs = [2,3]\n",
    "class_num_runs = [x * 10 for x in class_num_runs]\n",
    "for class_num_run in class_num_runs:\n",
    "    print(\"class num run: {}\".format(class_num_run))\n",
    "    train_data, test_data = load_processed_dataset(data_dir=\"/data0/user/gfhao/datasets/mnist\", batch_size=64, display=False, process=True, class_num=class_num_run)\n",
    "    print(\"datasets loaded!\")\n",
    "    for name in MODEL_NAME:\n",
    "        print(\"model name: \", name)\n",
    "        train_idx = 1\n",
    "        epoch = 9\n",
    "        print(\"train_idx: \", train_idx)\n",
    "        print(\"epoch: \", epoch)\n",
    "        fid_all, fid_matrix_all = compute_FID(test_data, train_idx, epoch, model_name=name, class_num=class_num_run, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_train_fid(middle_size, class_num, num_times):\n",
    "    max_sign = {\"middle_size\":round(middle_size, 0), \"class_num\":round(class_num, 0)}\n",
    "    max_val = 0\n",
    "    fid_path = generate_fid_path(middle_size, class_num, num_times)\n",
    "    fid_data = load_data_from_file(fid_path)\n",
    "    cnn_types = list(fid_data.keys())\n",
    "    cnn_type = cnn_types[2]  # 'CrossProductCNN'\n",
    "    for j in range(0, num_times):\n",
    "        fid_all = fid_data[cnn_type][j][0]\n",
    "        # 计算两个差值\n",
    "        diff1 = round(fid_all[0] - fid_all[1], 2)\n",
    "        diff2 = round(fid_all[2] - fid_all[3], 2)\n",
    "        if (diff1 * diff2) < 0:\n",
    "            if cnn_type in max_sign:\n",
    "                if abs(diff1 * diff2) > max_val:\n",
    "                    max_val = abs(diff1 * diff2)\n",
    "                    max_sign[cnn_type] = j+1\n",
    "                    max_sign[\"max_diff_fid\"] = fid_all\n",
    "                    max_sign[\"max_diff_fid_matrix\"] = fid_data[cnn_type][j][1]\n",
    "            else:\n",
    "                max_sign[cnn_type] = j+1\n",
    "                max_sign[\"max_diff_fid\"] = fid_all\n",
    "                max_sign[\"max_diff_fid_matrix\"] = fid_data[cnn_type][j][1]\n",
    "        else:\n",
    "            if cnn_type not in max_sign:\n",
    "                max_sign[cnn_type] = None\n",
    "                max_sign[\"max_diff_fid\"] = None\n",
    "                max_sign[\"max_diff_fid_matrix\"] = None\n",
    "    return max_sign\n",
    "\n",
    "def analyze_train_fid_positive(middle_size, class_num, num_times):\n",
    "    max_sign = {\"middle_size\":round(middle_size, 0), \"class_num\":round(class_num, 0)}\n",
    "    max_val = 0\n",
    "    fid_path = generate_fid_path(middle_size, class_num, num_times)\n",
    "    fid_data = load_data_from_file(fid_path)\n",
    "    cnn_types = list(fid_data.keys())\n",
    "    cnn_type = cnn_types[2]  # 'CrossProductCNN'\n",
    "    for j in range(0, num_times):\n",
    "        fid_all = fid_data[cnn_type][j][0]\n",
    "        diff1 = round(fid_all[0] - fid_all[1], 2)\n",
    "        diff2 = round(fid_all[2] - fid_all[3], 2)\n",
    "        if (diff1 * diff2) > 0:\n",
    "            if cnn_type in max_sign:\n",
    "                if abs(diff1 * diff2) > max_val:\n",
    "                    max_val = abs(diff1 * diff2)\n",
    "                    max_sign[cnn_type] = j+1\n",
    "                    max_sign[\"max_diff_fid_positive\"] = fid_all\n",
    "                    max_sign[\"max_diff_fid_matrix_positive\"] = fid_data[cnn_type][j][1]\n",
    "            else:\n",
    "                max_sign[cnn_type] = j+1\n",
    "                max_sign[\"max_diff_fid_positive\"] = fid_all\n",
    "                max_sign[\"max_diff_fid_matrix_positive\"] = fid_data[cnn_type][j][1]\n",
    "        else:\n",
    "            if cnn_type not in max_sign:\n",
    "                max_sign[cnn_type] = None\n",
    "                max_sign[\"max_diff_fid_positive\"] = None\n",
    "                max_sign[\"max_diff_fid_matrix_positive\"] = None\n",
    "    return max_sign\n",
    "\n",
    "def get_train_history_fid(out_dir, middle_size, class_num, epoch, num_times=100):\n",
    "    model_config = [8, 16, middle_size, middle_size * middle_size // 4]\n",
    "    # print(\"middle size:\", middle_size)\n",
    "    fid1, fid2, fid3, fid4, fid5 = 0,0,0,0,0\n",
    "    fid_all_5 = [0, 0, 0, 0, 0]\n",
    "    train_history_fid_epoch = {\"middle_size\":round(middle_size, 0), \"class_num\":round(class_num, 0), \"epoch\":round(epoch, 0)}\n",
    "    train_data, test_data = load_processed_dataset(data_dir=\"/data0/user/gfhao/datasets/mnist\", batch_size=64, display=False, process=True, class_num=class_num)\n",
    "    # print(\"datasets loaded!\")\n",
    "    for i in tqdm(range(1, num_times + 1), desc=\"Progress\"):\n",
    "        fid_all1, fid_matrix_all1 = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[0], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "        fid_all2, fid_matrix_all2 = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[1], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "        fid_all3, fid_matrix_all3 = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[2], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "        fid_all4, fid_matrix_all4 = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[3], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "        fid_all5, fid_matrix_all5 = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[4], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "\n",
    "        for j, fid_all_i in enumerate([fid_all1,fid_all2,fid_all3,fid_all4,fid_all5]):\n",
    "            diff1 = round(fid_all_i[0] - fid_all_i[1], 2)\n",
    "            diff2 = round(fid_all_i[2] - fid_all_i[3], 2)\n",
    "            if (diff1 * diff2) < 0:\n",
    "                fid_all_5[j] += 1\n",
    "\n",
    "    fid1 = fid_all_5[0]\n",
    "    fid2 = fid_all_5[1]\n",
    "    fid3 = fid_all_5[2]\n",
    "    fid4 = fid_all_5[3]\n",
    "    fid5 = fid_all_5[4]\n",
    "\n",
    "    data_to_save = {\n",
    "        MODEL_NAME[0]: fid1,\n",
    "        MODEL_NAME[1]: fid2,\n",
    "        MODEL_NAME[2]: fid3,\n",
    "        MODEL_NAME[3]: fid4,\n",
    "        MODEL_NAME[4]: fid5,\n",
    "    }\n",
    "    # Format filename with parameters\n",
    "    file_name = f\"./result/fid_train/fid_data_middle_size_{middle_size}_class_num_{class_num}_times{num_times}_epoch{epoch}.pkl\"\n",
    "    ensure_dir_exists(\"./result/fid_train/\")\n",
    "\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(data_to_save, file)\n",
    "    \n",
    "    return data_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num_runs = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50]\n",
    "class_num_runs = [x * 10 for x in class_num_runs]\n",
    "middle_size = 32\n",
    "num_times = 100\n",
    "'''\n",
    "for class_num_run in class_num_runs:\n",
    "    max_sign_fid = analyze_train_fid(middle_size, class_num_run, num_times)\n",
    "    print(max_sign_fid)\n",
    "    break\n",
    "'''\n",
    "import itertools\n",
    "import pickle\n",
    "class_num = 100\n",
    "max_sign_fid = analyze_train_fid(middle_size, class_num, num_times)\n",
    "max_diff_fid_matrix = max_sign_fid[\"max_diff_fid_matrix\"]\n",
    "keys = list(max_sign_fid.keys())\n",
    "print({k: max_sign_fid[k] for k in itertools.islice(max_sign_fid, 4)})\n",
    "# Format filename with parameters\n",
    "file_name = f\"./result/matrix/middle_size_{middle_size}_class_num_{class_num}_times{num_times}_max_diff_fid_matrix.pkl\"\n",
    "ensure_dir_exists(\"./result/matrix/\")\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(max_diff_fid_matrix, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "start_color_hex = \"#428abc\"  # blue\n",
    "end_color_hex = \"#ed4074\"   # red\n",
    "# Converting HEX colors to RGB\n",
    "start_color_rgb = mcolors.hex2color(start_color_hex)\n",
    "end_color_rgb = mcolors.hex2color(end_color_hex)\n",
    "white_rgb = mcolors.hex2color(\"#ffffff\")\n",
    "\n",
    "# Creating the colormap from white to blue (#428abc)\n",
    "white_to_blue_colors = [white_rgb, start_color_rgb]\n",
    "white_to_blue_cmap = mcolors.LinearSegmentedColormap.from_list(\"white_to_blue\", white_to_blue_colors)\n",
    "\n",
    "# Creating the colormap from white to red (#ed4074)\n",
    "white_to_red_colors = [white_rgb, end_color_rgb]\n",
    "white_to_red_cmap = mcolors.LinearSegmentedColormap.from_list(\"white_to_red\", white_to_red_colors)\n",
    "custom_cmap = [white_to_red_cmap, white_to_red_cmap, white_to_blue_cmap, white_to_blue_cmap]\n",
    "\n",
    "def plot_fid_matrix_max(middle_size, class_num, num_times):\n",
    "    file_path = f\"./result/matrix/middle_size_{middle_size}_class_num_{class_num}_times{num_times}_max_diff_fid_matrix.pkl\"\n",
    "    data = pd.read_pickle(file_path)\n",
    "    max_value_row1 = max(np.max(data[0]), np.max(data[1]))\n",
    "    max_value_row2 = max(np.max(data[2]), np.max(data[3]))\n",
    "    # custom_cmap = sns.diverging_palette(240, 0, s=80, l=55, n=9, as_cmap=True)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    # Plotting each matrix with the given titles\n",
    "    titles = [\n",
    "        \"FID matrix about dot of F1 group\",\n",
    "        \"FID matrix about number of F1 group\",\n",
    "        \"FID matrix about dot of F2 group\",\n",
    "        \"FID matrix about number of F2 group\"\n",
    "    ]\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        if i < len(data):\n",
    "            vmax = max_value_row1 if i < 2 else max_value_row2\n",
    "            # sns.heatmap(data[i], annot=False, cmap=\"coolwarm\", ax=ax, vmin=0, vmax=vmax)\n",
    "            sns.heatmap(data[i], annot=False, cmap=custom_cmap[i], ax=ax, vmin=0, vmax=vmax)\n",
    "            ax.set_title(titles[i], fontsize=20, pad=20)\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"\")\n",
    "        else:\n",
    "            ax.axis('off')  # Hide axis if there's no corresponding matrix\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def plot_fid_matrix_positive_max(middle_size, class_num, num_times):\n",
    "    file_path = f\"./result/matrix/middle_size_{middle_size}_class_num_{class_num}_times{num_times}_max_diff_fid_matrix_positive.pkl\"\n",
    "    data = pd.read_pickle(file_path)\n",
    "    max_value_row1 = max(np.max(data[0]), np.max(data[1]))\n",
    "    max_value_row2 = max(np.max(data[2]), np.max(data[3]))\n",
    "    # custom_cmap = sns.diverging_palette(240, 0, s=80, l=55, n=9, as_cmap=True)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    # Plotting each matrix with the given titles\n",
    "    titles = [\n",
    "        \"FID matrix about dot of F1 group\",\n",
    "        \"FID matrix about number of F1 group\",\n",
    "        \"FID matrix about dot of F2 group\",\n",
    "        \"FID matrix about number of F2 group\"\n",
    "    ]\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        if i < len(data):\n",
    "            vmax = max_value_row1 if i < 2 else max_value_row2\n",
    "            # sns.heatmap(data[i], annot=False, cmap=\"coolwarm\", ax=ax, vmin=0, vmax=vmax)\n",
    "            sns.heatmap(data[i], annot=False, cmap=custom_cmap[i], ax=ax, vmin=0, vmax=vmax)\n",
    "            ax.set_title(titles[i], fontsize=20, pad=20)\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"\")\n",
    "        else:\n",
    "            ax.axis('off')  # Hide axis if there's no corresponding matrix\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num_runs = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50]\n",
    "class_num_runs = [x * 10 for x in class_num_runs]\n",
    "middle_size = 32\n",
    "num_times = 100\n",
    "class_num = 100\n",
    "fig = plot_fid_matrix_max(middle_size, class_num, num_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Colors in HEX format\n",
    "start_color_hex = \"#428abc\"  # blue\n",
    "end_color_hex = \"#ed4074\"   # red\n",
    "middle_color_hex = \"#ffffff\" # white\n",
    "middle_middle_color_hex = \"#d9d9d9\" # gray\n",
    "\n",
    "# Converting HEX colors to RGB\n",
    "start_color_rgb = mcolors.hex2color(start_color_hex)\n",
    "end_color_rgb = mcolors.hex2color(end_color_hex)\n",
    "middle_color_rgb = mcolors.hex2color(middle_color_hex)\n",
    "middle_middle_color_rgb = mcolors.hex2color(middle_middle_color_hex)\n",
    "\n",
    "# Creating the colormap\n",
    "custom_colors = [\n",
    "    start_color_rgb, \n",
    "    middle_middle_color_rgb, \n",
    "    middle_color_rgb,  # Adding more white to expand the middle range\n",
    "    middle_middle_color_rgb, \n",
    "    end_color_rgb\n",
    "]\n",
    "custom_cmap_expanded_white = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", custom_colors)\n",
    "\n",
    "class NeuronExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(NeuronExtractor, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        _ = self.model(x)\n",
    "        feat_cat = torch.cat([self.model.feature_map[0], self.model.feature_map[1]], dim=1)\n",
    "        activations = feat_cat\n",
    "        return activations\n",
    "\n",
    "def plot_shap(datasets, train_idx, epoch, model_name='', class_num=100, lambda_sparse=0.01, model_config=[], out_dir='', num=4):\n",
    "    middle_size = model_config[2]\n",
    "    test_net = initialize_model(model_name, class_num, model_config, lambda_sparse)\n",
    "    filename = f\"model_{train_idx}_epoch_{epoch}.pt\"\n",
    "    path = get_path(out_dir + \"model_weights\", middle_size, class_num, model_name, filename)\n",
    "    # print(path)\n",
    "    ensure_dir_exists(path)\n",
    "    test_net.load_state_dict(torch.load(path))\n",
    "    model = test_net\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    test_loader = datasets\n",
    "    inputs, _ = next(iter(test_loader))\n",
    "    inputs.requires_grad = True\n",
    "    model.train()\n",
    "    explainers = shap.DeepExplainer(NeuronExtractor(model), inputs)\n",
    "    shap_values = explainers.shap_values(inputs[0:num])\n",
    "    # shap.summary_plot(shap_values, inputs[0:1])\n",
    "    shap_numpy = [np.transpose(s, (0, 2, 3, 1)) for s in shap_values]\n",
    "    input_numpy = np.transpose(inputs[0:num].detach().numpy(), (0, 2, 3, 1))\n",
    "    colors = ['#397AA3', 'white', '#E71555']\n",
    "    colormap = LinearSegmentedColormap.from_list('custom_blue_white_red', colors)\n",
    "    shap.image_plot(shap_numpy, -input_numpy, cmap=colormap)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/data0/user/gfhao/code_cpm/ServerCode/output/\"\n",
    "\n",
    "middle_size = 32\n",
    "model_config = [8, 16, middle_size, middle_size * middle_size // 4]\n",
    "print(\"middle size:\", middle_size)\n",
    "# class_num_runs = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50]\n",
    "class_num = 100\n",
    "train_data, test_data = load_processed_dataset(data_dir=\"/data0/user/gfhao/datasets/mnist\", batch_size=64, display=False, process=True, class_num=class_num)\n",
    "train_idx = 4\n",
    "name = MODEL_NAME[2]\n",
    "epoch = 9\n",
    "plot_shap(test_data, train_idx, epoch, model_name=name, class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "colors = ['#397AA3', 'white', '#E71555']\n",
    "colormap = LinearSegmentedColormap.from_list('custom_blue_white_red', colors)\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "def plot_color_gradients(cmap_category, cmap_list):\n",
    "    nrows = len(cmap_list)\n",
    "    figh = 0.35 + 0.15 + (nrows + (nrows-1)*0.1)*0.22\n",
    "    fig, axs = plt.subplots(nrows=nrows, figsize=(6.4, figh))\n",
    "    fig.subplots_adjust(top=1-.35/figh, bottom=.15/figh, left=0.2, right=0.99)\n",
    "\n",
    "    axs.imshow(gradient, aspect='auto', cmap=cmap_list[0])\n",
    "    axs.text(-.01, .5, cmap_list[0].name, va='center', ha='right', fontsize=10, transform=axs.transAxes)\n",
    "    axs.set_axis_off()\n",
    "plot_color_gradients('Custom', [colormap])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/data0/user/gfhao/code_cpm/ServerCode/output/\"\n",
    "class_num = 100\n",
    "for middle_size in [16,32,64,128,256]:\n",
    "    for epoch in range(0,10):\n",
    "        if middle_size == 16 and epoch == 0:\n",
    "            continue\n",
    "        else:\n",
    "            print(\"middle_size: {}, epoch: {}\".format(middle_size, epoch))\n",
    "            percent = get_train_history_fid(out_dir, middle_size, class_num, epoch, num_times=100)\n",
    "            print(percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "class_num = 100\n",
    "num_times = 100\n",
    "all_fid_trend = []\n",
    "for middle_size in [16,32,64,128,256]:\n",
    "    train_fid_epoch_trend = {\"middle_size\":round(middle_size, 0), \"data\":[]}\n",
    "    for epoch in range(0,10):\n",
    "        train_fid_epoch = {\"epoch\":round(epoch, 0)}\n",
    "        file_name = f\"./result/fid_train/fid_data_middle_size_{middle_size}_class_num_{class_num}_times{num_times}_epoch{epoch}.pkl\"\n",
    "        ensure_dir_exists(\"./result/fid_train/\")\n",
    "        train_fid_epoch[\"data\"] = load_data_from_file(file_name)\n",
    "        train_fid_epoch_trend[\"data\"].append(train_fid_epoch)\n",
    "    all_fid_trend.append(train_fid_epoch_trend)\n",
    "# Format filename with parameters\n",
    "file_name = f\"./result/fid_train_all/all_fid_trend_class_num_{class_num}_times{num_times}_all_epoch.pkl\"\n",
    "ensure_dir_exists(\"./result/fid_train_all/\")\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(all_fid_trend, file)\n",
    "print(all_fid_trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import textwrap\n",
    "\n",
    "\n",
    "num_times = 100\n",
    "path_fid_data = f\"./result/fid_all/all_fid_data.pkl\"\n",
    "path_fid_trend_100 = f\"./result/fid_train_all/all_fid_trend_class_num_100_times{num_times}_all_epoch.pkl\"\n",
    "path_fid_trend_500 = f\"./result/fid_train_all/all_fid_trend_class_num_500_times{num_times}_all_epoch.pkl\"\n",
    "with open(path_fid_data, \"rb\") as f:\n",
    "    fid_data = pickle.load(f)\n",
    "\n",
    "with open(path_fid_trend_100, \"rb\") as f:\n",
    "    fid_trend_100_data = pickle.load(f)\n",
    "\n",
    "with open(path_fid_trend_500, \"rb\") as f:\n",
    "    fid_trend_500_data = pickle.load(f)\n",
    "\n",
    "\n",
    "# Function to sort legend labels by the numerical part of the label\n",
    "def sort_legend(ax):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    processed_labels = [label.split(\"(\")[-1].rstrip(\")\") for label in labels]\n",
    "    labels_sorted = sorted(zip(processed_labels, handles), key=lambda x: int(x[0].split(\"_\")[-1]))\n",
    "    return [label for label, _ in labels_sorted], [handle for _, handle in labels_sorted]\n",
    "\n",
    "# Create a new figure with adjusted dimensions\n",
    "fig_combined_sorted_legend = plt.figure(figsize=(12, 20), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(4, 2, height_ratios=[4, 4, 3, 3])\n",
    "ax1_sorted_legend = fig_combined_sorted_legend.add_subplot(gs[0, :])  # First row\n",
    "ax2_sorted_legend = fig_combined_sorted_legend.add_subplot(gs[1, :])  # Second row\n",
    "ax3_sorted_legend = fig_combined_sorted_legend.add_subplot(gs[2, :-1])  # Third row, first column\n",
    "ax4_sorted_legend = fig_combined_sorted_legend.add_subplot(gs[2, -1])  # Third row,\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Set font size for the legends and labels\n",
    "fontsize = 14\n",
    "legend_fontsize = fontsize\n",
    "label_fontsize = fontsize\n",
    "\n",
    "class_num_runs = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,25,30,35,40,45,50]\n",
    "class_num_runs = [x * 10 for x in class_num_runs]\n",
    "adjusted_class_nums = [num if num < 200 else int(200 + (num - 200) / 2) for num in class_num_runs]\n",
    "df_data = []\n",
    "for element in fid_data:\n",
    "    for middle_size, models in element.items():\n",
    "        for model, percentages in models.items():\n",
    "            if model in ['SimpleCNN', 'DotProductCNN', 'CrossProductCNN']:\n",
    "                for class_num_index, percentage in enumerate(percentages):\n",
    "                    df_data.append({\n",
    "                        'Middle Size': middle_size,\n",
    "                        'Model': model,\n",
    "                        'Class Number': (class_num_index + 2)*10,  # Starting from class_num 2\n",
    "                        'Percentage': percentage\n",
    "                    })\n",
    "df = pd.DataFrame(df_data)\n",
    "\n",
    "all_middle_sizes = sorted(set(df['Middle Size'].unique()))\n",
    "cmap = plt.cm.get_cmap('Accent', len(all_middle_sizes))\n",
    "color_map = {size: cmap(i) for i, size in enumerate(all_middle_sizes)}\n",
    "\n",
    "models = ['DotProductCNN', 'CrossProductCNN']\n",
    "\n",
    "for i, (model, ax) in enumerate(zip(models, [ax1_sorted_legend, ax2_sorted_legend])):\n",
    "    model_data = df[df['Model'] == model]\n",
    "    for j, middle_size in enumerate(sorted(model_data['Middle Size'].unique())):\n",
    "        middle_size_data = model_data[model_data['Middle Size'] == middle_size]\n",
    "        color = color_map[middle_size]\n",
    "        ax.plot(adjusted_class_nums, middle_size_data['Percentage'], label=middle_size, color=color)  # , marker='o'\n",
    "    ax.set_title(\"\\n\".join(textwrap.wrap(f'FID Score for {model}', width=30)), fontsize=label_fontsize)\n",
    "    ax.set_xlabel('Class Number', fontsize=label_fontsize)\n",
    "    ax.set_ylabel('FID Score', fontsize=label_fontsize)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.grid(True)\n",
    "    if i == 0:\n",
    "        sorted_labels, sorted_handles = sort_legend(ax)\n",
    "        ax.legend(sorted_handles, sorted_labels, fontsize=legend_fontsize)\n",
    "    ax.set_xticks(adjusted_class_nums, class_num_runs, rotation=45, fontsize=label_fontsize)\n",
    "    ylabels = ax.get_yticks()\n",
    "    ax.set_yticklabels(ylabels, fontsize=label_fontsize)\n",
    "\n",
    "# Function to plot FID trend data\n",
    "def plot_fid_train_all_one_fig(data, class_num):\n",
    "    epochs = range(10)\n",
    "    middle_sizes = set(item['middle_size'] for item in data)\n",
    "    sorted_middle_sizes = sorted(middle_sizes)\n",
    "    plot_data_by_size = {size: [] for size in middle_sizes}\n",
    "    for item in data:\n",
    "        size = item['middle_size']\n",
    "        model_data = [epoch_data['data']['CrossProductCNN'] for epoch_data in item['data']]\n",
    "        plot_data_by_size[size].append(model_data)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    cmap = plt.cm.get_cmap('Accent', len(sorted_middle_sizes))\n",
    "    for i, size in enumerate(sorted_middle_sizes):\n",
    "        all_config_data = plot_data_by_size[size]\n",
    "        avg_data = [sum(config_data[j] for config_data in all_config_data) / len(all_config_data) for j in epochs]\n",
    "        plt.plot(epochs, avg_data, label=f'Middle Size {size}', color=color_map[f'middle_size_{size}'])\n",
    "\n",
    "    plt.title(f'FID Trend for CrossProductCNN, Class Num: {class_num}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('FID Score')\n",
    "    plt.ylim([0, 100])\n",
    "    plt.xticks(epochs)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig_trend_100 = plot_fid_train_all_one_fig(fid_trend_100_data, 100)\n",
    "fig_trend_500 = plot_fid_train_all_one_fig(fid_trend_500_data, 500)\n",
    "\n",
    "# Copy plots from the original figures to the new layout\n",
    "\n",
    "for src_fig, ax in zip([fig_trend_100, fig_trend_500], [ax3_sorted_legend, ax4_sorted_legend]):\n",
    "    src_ax = src_fig.axes[0]\n",
    "    for line in src_ax.get_lines():\n",
    "        ax.plot(line.get_xdata(), line.get_ydata(), label=line.get_label(), color=line.get_color())\n",
    "    # ax.set_title(src_ax.get_title(), fontsize=label_fontsize)\n",
    "    ax.set_title(\"\\n\".join(textwrap.wrap(src_ax.get_title(), width=30)), fontsize=label_fontsize)\n",
    "    ax.set_xlabel(src_ax.get_xlabel(), fontsize=label_fontsize)\n",
    "    ax.set_ylabel(src_ax.get_ylabel(), fontsize=label_fontsize)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_xticks(range(10))\n",
    "    ax.set_xticklabels([str(x+1) for x in range(10)], fontsize=label_fontsize)\n",
    "    ylabels = ax.get_yticks()\n",
    "    ax.set_yticklabels(ylabels, fontsize=label_fontsize)\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "# Display the final combined figure with sorted legends\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/data0/user/gfhao/code_cpm/ServerCode/output/\"\n",
    "middle_size = 16\n",
    "class_num = 100\n",
    "num_times = 100\n",
    "fig1 = plot_fig1_loss_train_test(out_dir, middle_size, class_num, num_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_crossproduct_symmetry_and_asymmetric(out_dir, middle_size, class_num, epoch, num_times=100):\n",
    "    model_config = [8, 16, middle_size, middle_size * middle_size // 4]\n",
    "    # print(\"middle size:\", middle_size)\n",
    "    fid_all_2 = [[0,0,0,0], [0,0,0,0]]\n",
    "    train_history_fid_epoch = {\"middle_size\":round(middle_size, 0), \"class_num\":round(class_num, 0), \"epoch\":round(epoch, 0)}\n",
    "    train_data, test_data = load_processed_dataset(data_dir=\"/data0/user/gfhao/datasets/mnist\", batch_size=64, display=False, process=True, class_num=class_num)\n",
    "    # print(\"datasets loaded!\")\n",
    "    for i in tqdm(range(1, num_times + 1), desc=\"Progress\"):\n",
    "        fid_all_symmetry, fid_matrix_symmetry = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[2], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "        fid_all_asymmetric, fid_matrix_asymmetric = compute_FID(test_data, i, epoch, model_name=MODEL_NAME[5], class_num=class_num, lambda_sparse=0.01, model_config=model_config, out_dir=out_dir)\n",
    "\n",
    "        # 计算两个差值\n",
    "        for j, fid_all_i in enumerate([fid_all_symmetry,fid_all_asymmetric]):\n",
    "            diff1 = round(fid_all_i[0] - fid_all_i[1], 2)\n",
    "            diff2 = round(fid_all_i[2] - fid_all_i[3], 2)\n",
    "            if diff1 > 0 and diff2 > 0:\n",
    "                fid_all_2[j][0] += 1\n",
    "            elif diff1 > 0 and diff2 < 0:\n",
    "                fid_all_2[j][1] += 1\n",
    "            elif diff1 < 0 and diff2 > 0:\n",
    "                fid_all_2[j][2] += 1\n",
    "            elif diff1 < 0 and diff2 < 0:\n",
    "                fid_all_2[j][3] += 1\n",
    "\n",
    "    fid1 = fid_all_2[0]\n",
    "    fid2 = fid_all_2[1]\n",
    "\n",
    "    data_to_save = {\n",
    "        MODEL_NAME[2]: fid1,\n",
    "        MODEL_NAME[5]: fid2,\n",
    "    }\n",
    "    # Format filename with parameters\n",
    "    file_name = f\"./result/fid_crossproduct_contrast/fid_crossproduct_contrast_middle_size_{middle_size}_class_num_{class_num}_times{num_times}_epoch{epoch}.pkl\"\n",
    "    ensure_dir_exists(\"./result/fid_crossproduct_contrast/\")\n",
    "\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(data_to_save, file)\n",
    "    \n",
    "    return data_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/data0/user/gfhao/code_cpm/ServerCode/output/\"\n",
    "class_num = 100\n",
    "for middle_size in [16,32,64,128,256]:\n",
    "    for epoch in range(0,10):\n",
    "        print(\"middle_size: {}, epoch: {}\".format(middle_size, epoch))\n",
    "        percent = contrast_crossproduct_symmetry_and_asymmetric(out_dir, middle_size, class_num, epoch, num_times=100)\n",
    "        print(percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_CrossProduct_history_contrast_loss_train_test(out_dir, middle_size, class_num, num_times=100):\n",
    "    loss_train_test_all1, loss_train_test_all2, loss_train_test_all3, loss_train_test_all4, loss_train_test_all5 = [], [], [], [], []\n",
    "    for i in range(1, num_times + 1):\n",
    "        path1 = generate_file_path(out_dir, middle_size, class_num, MODEL_NAME[2], i)\n",
    "        path2 = generate_file_path(out_dir, middle_size, class_num, MODEL_NAME[5], i)\n",
    "\n",
    "        loss_train_test_all1.append(load_data_from_file(path1))\n",
    "        loss_train_test_all2.append(load_data_from_file(path2))\n",
    "\n",
    "    avg1, var_up_1, var_lower_1 = compute_stats(loss_train_test_all1)\n",
    "    avg2, var_up_2, var_lower_2 = compute_stats(loss_train_test_all2)\n",
    "\n",
    "    data_to_save = {\n",
    "        MODEL_NAME[2]: [loss_train_test_all1, avg1, var_up_1, var_lower_1],\n",
    "        MODEL_NAME[5]: [loss_train_test_all2, avg2, var_up_2, var_lower_2],\n",
    "    }\n",
    "\n",
    "    return data_to_save\n",
    "\n",
    "def plot_CrossProduct_variance_comparison_error_bars(data, metric_index, metric_name, ax):\n",
    "    epochs = np.arange(1, EPOCH+1)\n",
    "    colors = ['green', 'blue']  # Distinct colors for each CNN type\n",
    "    for idx, (cnn_type, color) in enumerate(zip(['CrossProductCNN', 'CrossProductAsymmetricCNN'], colors)):\n",
    "        means = data[cnn_type][1][metric_index]\n",
    "        lower_bounds = data[cnn_type][2][metric_index]\n",
    "        upper_bounds = data[cnn_type][3][metric_index]\n",
    "        error = [means - lower_bounds, upper_bounds - means]  # Asymmetric error bars\n",
    "        ax.errorbar(epochs, means, yerr=error, fmt='-o', color=color, ecolor=color, elinewidth=2, capsize=4, label=f'{cnn_type}')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(f'{metric_name} with Variance')\n",
    "    title = f'Comparison of {metric_name} with Variance Across CNN Types'\n",
    "    ax.set_title(\"\\n\".join(wrap(title, 40)))  # Wrap title based on the plot width\n",
    "    ax.legend(loc='center right')  # Move legend to the center right\n",
    "    ax.grid(True)  # Enable grid lines\n",
    "\n",
    "\n",
    "def plot_CrossProduct_fig1_loss_train_test(out_dir, middle_size, class_num, num_times=100):\n",
    "    # Create a figure with subplots for each metric's variance comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    # Iterate over each metric to create the plots with error bars\n",
    "    metrics = ['Loss', 'Training Accuracy', 'Validation Accuracy']\n",
    "    data_instance = evaluate_CrossProduct_history_contrast_loss_train_test(out_dir=out_dir, middle_size=middle_size, class_num=class_num, num_times=num_times)\n",
    "    for i, metric_name in enumerate(metrics):\n",
    "        plot_CrossProduct_variance_comparison_error_bars(data_instance, i, metric_name, axes[i])\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/data0/user/gfhao/code_cpm/ServerCode/output/\"\n",
    "class_num = 100\n",
    "num_times = 100\n",
    "fig1 = plot_CrossProduct_fig1_loss_train_test(out_dir, 16, class_num, num_times)\n",
    "fig2 = plot_CrossProduct_fig1_loss_train_test(out_dir, 32, class_num, num_times)\n",
    "fig3 = plot_CrossProduct_fig1_loss_train_test(out_dir, 64, class_num, num_times)\n",
    "fig4 = plot_CrossProduct_fig1_loss_train_test(out_dir, 128, class_num, num_times)\n",
    "fig5 = plot_CrossProduct_fig1_loss_train_test(out_dir, 256, class_num, num_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "class_num = 100\n",
    "num_times = 100\n",
    "all_fid_trend = []\n",
    "for middle_size in [16,32,64,128,256]:\n",
    "    train_fid_epoch_trend = {\"middle_size\":round(middle_size, 0), \"data\":[]}\n",
    "    for epoch in range(0,10):\n",
    "        train_fid_epoch = {\"epoch\":round(epoch, 0)}\n",
    "        file_name = f\"./result/fid_crossproduct_contrast/fid_crossproduct_contrast_middle_size_{middle_size}_class_num_{class_num}_times{num_times}_epoch{epoch}.pkl\"\n",
    "        ensure_dir_exists(\"./result/fid_crossproduct_contrast/\")\n",
    "        train_fid_epoch[\"data\"] = load_data_from_file(file_name)\n",
    "        train_fid_epoch_trend[\"data\"].append(train_fid_epoch)\n",
    "    all_fid_trend.append(train_fid_epoch_trend)\n",
    "# Format filename with parameters\n",
    "file_name = f\"./result/fid_crossproduct_contrast_all/all_fid_crossproduct_contrast_trend_class_num_{class_num}_times{num_times}_all_epoch.pkl\"\n",
    "ensure_dir_exists(\"./result/fid_crossproduct_contrast_all/\")\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(all_fid_trend, file)\n",
    "print(all_fid_trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_crosspoduct_fid_train_all(class_num = 100):\n",
    "    num_times = 100\n",
    "    file_name = f\"./result/fid_crossproduct_contrast_all/all_fid_crossproduct_contrast_trend_class_num_{class_num}_times{num_times}_all_epoch.pkl\"\n",
    "    data = pd.read_pickle(file_name)\n",
    "    model_types = list(data[0]['data'][0]['data'].keys())\n",
    "    epochs = range(10)\n",
    "    middle_sizes = set(item['middle_size'] for item in data)\n",
    "    plot_data_by_size = {size: {model: [] for model in model_types} for size in middle_sizes}\n",
    "    for item in data:\n",
    "        size = item['middle_size']\n",
    "        for model in model_types:\n",
    "            model_data = [epoch_data['data'][model] for epoch_data in item['data']]\n",
    "            plot_data_by_size[size][model].append(model_data)\n",
    "    sorted_middle_sizes = sorted(middle_sizes)\n",
    "    color_map = {model: color for model, color in zip(model_types, ['green', 'blue'])}\n",
    "    # Plotting with specified requirements\n",
    "    fig, axs = plt.subplots(len(sorted_middle_sizes), 1, figsize=(8, 6*len(sorted_middle_sizes)))\n",
    "    for i, size in enumerate(sorted_middle_sizes):\n",
    "        for model in model_types:\n",
    "            # Plotting each model type with its assigned color\n",
    "            for config_data in plot_data_by_size[size][model]:\n",
    "                sums = [subarray[1] + subarray[2] for subarray in config_data]\n",
    "                # axs[i].plot(epochs, sums, label=model if i == 0 else \"\", color=color_map[model])\n",
    "                axs[i].plot(epochs, sums, label=model, color=color_map[model])\n",
    "        axs[i].set_title(f'Middle Size {size}')\n",
    "        axs[i].set_xlabel('Epoch')\n",
    "        axs[i].set_ylabel('FID percentage')\n",
    "        axs[i].set_ylim([0, 100])  # Setting y-axis range\n",
    "        axs[i].set_xticks(epochs)  # Setting x-axis ticks\n",
    "        axs[i].set_xticklabels([r+1 for r in range(10)])\n",
    "        axs[i].legend()  # Adding legend only to the first plot\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_crosspoduct_fid_train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def plot_crosspoduct_isfixed_fid_train_all():\n",
    "    class_num = 100\n",
    "    num_times = 100\n",
    "    file_name = f\"./result/fid_crossproduct_contrast_all/all_fid_crossproduct_contrast_trend_class_num_{class_num}_times{num_times}_all_epoch.pkl\"\n",
    "    data = pd.read_pickle(file_name)\n",
    "    model_types = list(data[0]['data'][0]['data'].keys())\n",
    "    epochs = range(10)\n",
    "    middle_sizes = set(item['middle_size'] for item in data)\n",
    "    plot_data_by_size = {size: {model: [] for model in model_types} for size in middle_sizes}\n",
    "    for item in data:\n",
    "        size = item['middle_size']\n",
    "        for model in model_types:\n",
    "            model_data = [epoch_data['data'][model] for epoch_data in item['data']]\n",
    "            plot_data_by_size[size][model].append(model_data)\n",
    "    sorted_middle_sizes = sorted(middle_sizes)\n",
    "    color_map = {model: color for model, color in zip(model_types, ['green', 'blue'])}\n",
    "    tags = ['F1_dot > F1_number && F2_dot > F2_number', \n",
    "    'F1_dot > F1_number && F2_dot < F2_number', \n",
    "    'F1_dot < F1_number && F2_dot > F2_number', \n",
    "    'F1_dot < F1_number && F2_dot < F2_number']\n",
    "    colors_blue=['#5789ba','#18305e','#3961a0','#8cb5d3']\n",
    "    colors_green=['#9dcdc2','#335635','#4e8753','#c8e2de']\n",
    "    colors = [colors_green, colors_blue]\n",
    "    # Creating legend items\n",
    "    # legend_handles = [Patch(color=color, label=tag) for color, tag in zip(colors_green + colors_blue, tags * 2)]\n",
    "    # 创建图例项\n",
    "    legend_handles = []\n",
    "    for i in range(4):\n",
    "        patch1 = Patch(color=colors_green[i], label=tags[i])\n",
    "        patch2 = Patch(color=colors_blue[i], label=tags[i])\n",
    "        legend_handles.append(patch1)\n",
    "        legend_handles.append(patch2)\n",
    "\n",
    "    bar_width = 0.25\n",
    "    bottom_y = [0] * 10\n",
    "    # Plotting with specified requirements\n",
    "    fig, axs = plt.subplots(len(sorted_middle_sizes), 1, figsize=(8, 6*len(sorted_middle_sizes)))\n",
    "    for i, size in enumerate(sorted_middle_sizes):\n",
    "        for j, model in enumerate(model_types):\n",
    "            # Plotting each model type with its assigned color\n",
    "            for config_data in plot_data_by_size[size][model]:\n",
    "                bar_positions = [k + j * bar_width for k in range(10)]\n",
    "                transposed_data = list(map(list, zip(*config_data)))\n",
    "                bottom_y = [0] * 10\n",
    "                for ind in range(len(transposed_data)):\n",
    "                    y = transposed_data[ind]\n",
    "                    if j == 0:\n",
    "                        axs[i].bar(bar_positions, y, width=bar_width, bottom=bottom_y,label=tags[ind], color=colors[j][ind])\n",
    "                    else:\n",
    "                        axs[i].bar(bar_positions, y, width=bar_width, bottom=bottom_y,label='', color=colors[j][ind])\n",
    "                    bottom_y = [(a+b) for a, b in zip(y, bottom_y)]\n",
    "                # axs[i].plot(epochs, sums, label=model if i == 0 else \"\", color=color_map[model])\n",
    "                # axs[i].stackplot(bar_positions, transposed_data, color=color_map[model])\n",
    "        axs[i].set_title(f'Middle Size {size}')\n",
    "        axs[i].set_xlabel('Epoch')\n",
    "        axs[i].set_ylabel('FID percentage')\n",
    "        axs[i].set_ylim([0, 100])  # Setting y-axis range\n",
    "        axs[i].set_xticks([r + 0.5 * bar_width for r in range(10)])  # Setting x-axis ticks\n",
    "        axs[i].set_xticklabels([r+1 for r in range(10)])\n",
    "        \n",
    "        '''\n",
    "        if i == 0:\n",
    "            for leg_i in range(8):\n",
    "                axs[i].legend(handles=legend_handles[leg_i])  # Adding legend only to the first plot\n",
    "        '''\n",
    "        '''\n",
    "        axs[i].legend(handles=legend_handles, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        '''\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_crosspoduct_isfixed_fid_train_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
